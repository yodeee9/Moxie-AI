{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n"
     ]
    }
   ],
   "source": [
    "!apt-get install python3-dev graphviz libgraphviz-dev pkg-config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/00-langgraph-intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.1.42 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-openai==0.1.3 \n",
    "!pip install -qU langchain-core==0.1.42\n",
    "!pip install -qU langgraph==0.0.37\n",
    "!pip install -qU langchainhub==0.1.15 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.agents import AgentFinish\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder,HumanMessagePromptTemplate\n",
    "import operator\n",
    "import os\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT_AUGMENTED_JP = \"\"\"\n",
    "You are an advanced apartment search assistant. Your goal is to assist users in finding apartments based on their preferences. You have access to several tools that can help gather user preferences, check if all necessary preferences are provided, and search for apartments using a Dummy API. Use these tools as needed to complete the user's request.\n",
    "\n",
    "Available tools:\n",
    "1. ask_user: Use this tool to communicate information or ask questions to the user.\n",
    "2. search: Use this tool to search for apartments based on the provided preferences. Ensure all preferences are gathered before calling this tool.\n",
    "\n",
    "Note: If you use tools, make sure the response object format is correct.\n",
    "\n",
    "Instructions:\n",
    "- If any preference is missing, use tool:[ask_user] to ask the user for the missing information.\n",
    "- Once all preferences are provided, use too:[search] to find suitable apartments.\n",
    "- Ensure clear and polite communication with the user at all times.\n",
    "\n",
    "For example, if the user asked like \"I am looking for apartment\", you can ask \"Do you have any preferences for the apartment?\" using the tool:[ask_user]. \n",
    "\"\"\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    agent_out: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "@tool(\"ask_user\")\n",
    "def statement_for_user(statement: str):\n",
    "    \"\"\"Send a statement to the user.\"\"\"\n",
    "    return statement\n",
    "\n",
    "@tool(\"search\")\n",
    "def search_apartments(isCityView: Optional[bool] = None, schools: Optional[str] = None, bedsMax: Optional[int] = None, maxPrice: Optional[int] = None, location: Optional[str] = None):\n",
    "    \"\"\"Search for apartments based on user preferences.\"\"\"\n",
    "    params = {\n",
    "        \"isCityView\": isCityView,\n",
    "        \"schools\": schools,\n",
    "        \"bedsMax\": bedsMax,\n",
    "        \"maxPrice\": maxPrice,\n",
    "        \"location\": location\n",
    "    }\n",
    "    filtered_params = {k: v for k, v in params.items() if v is not None}\n",
    "    return 'dummy response'\n",
    "\n",
    "@tool(\"final_answer\")\n",
    "def final_answer_tool(\n",
    "    answer: str,\n",
    "    source: str\n",
    "):\n",
    "    \"\"\"Returns a natural language response to the user in `answer`, and a\n",
    "    `source` which provides citations for where this information came from.\n",
    "    \"\"\"\n",
    "    return \"\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",SYSTEM_PROMPT_AUGMENTED_JP),\n",
    "        MessagesPlaceholder(variable_name='chat_history', optional=True), \n",
    "        HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), \n",
    "        MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_agent_runnable = create_openai_tools_agent(\n",
    "    llm=llm,\n",
    "    tools=[statement_for_user,search_apartments],\n",
    "    prompt=primary_assistant_prompt\n",
    ")\n",
    "\n",
    "def run_query_agent(state: list):\n",
    "    print(\"> run_query_agent\")\n",
    "    agent_out = query_agent_runnable.invoke(state)\n",
    "    return {\"agent_out\": agent_out}\n",
    "\n",
    "def execute_ask_user_preferences(state: list):\n",
    "    print(\"> run_ask_user_preferences\")\n",
    "    action = state[\"agent_out\"]\n",
    "    print(f\"action: {action}\")\n",
    "    tool_call = action[-1].message_log[-1].additional_kwargs[\"tool_calls\"][-1]\n",
    "    print(f\"tool_call: {tool_call}\")\n",
    "    out = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "    print(f\"out: {out}\")\n",
    "    return {\"intermediate_steps\": [{\"statement\": str(out)}]}\n",
    "\n",
    "def execute_search_apartments(state: list):\n",
    "    print(\"> execute_search\")\n",
    "    action = state[\"agent_out\"]\n",
    "    tool_call = action[-1].message_log[-1].additional_kwargs[\"tool_calls\"][-1]\n",
    "    out = search_apartments.invoke(\n",
    "        json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "    )\n",
    "    return {\"intermediate_steps\": [{\"search\": str(out)}]}\n",
    "\n",
    "def router(state: list):\n",
    "    print(\"> router\")\n",
    "    if isinstance(state[\"agent_out\"], list):\n",
    "        return state[\"agent_out\"][-1].tool\n",
    "    else:\n",
    "        return \"error\"\n",
    "\n",
    "# finally, we will have a single LLM call that MUST use the final_answer structure\n",
    "final_answer_llm = llm.bind_tools([final_answer_tool], tool_choice=\"final_answer\")\n",
    "\n",
    "# # we use the same forced final_answer LLM call to handle incorrectly formatted\n",
    "# # output from our query_agent\n",
    "def handle_error(state: list):\n",
    "    print(\"> handle_error\")\n",
    "    query = state[\"input\"]\n",
    "    prompt = f\"\"\"You are a helpful assistant, answer the user's question.\n",
    "\n",
    "    QUESTION: {query}\n",
    "    \"\"\"\n",
    "    out = final_answer_llm.invoke(prompt)\n",
    "    function_call = out.additional_kwargs[\"tool_calls\"][-1][\"function\"][\"arguments\"]\n",
    "    return {\"agent_out\": function_call}\n",
    "\n",
    "graph.add_node(\"query_agent\", run_query_agent)\n",
    "graph.add_node(\"search\", execute_search_apartments)\n",
    "graph.add_node(\"error\", handle_error)\n",
    "graph.add_node(\"ask_user\", execute_ask_user_preferences)\n",
    "graph.set_entry_point(\"query_agent\")\n",
    "graph.add_conditional_edges(\n",
    "    start_key=\"query_agent\",  # where in graph to start\n",
    "    condition=router,  # function to determine which node is called\n",
    "    conditional_edge_mapping={\n",
    "        \"search\": \"search\",\n",
    "        \"ask_user\": \"ask_user\",\n",
    "        \"error\": \"error\",\n",
    "        \"final_answer\": END\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"error\", END)\n",
    "graph.add_edge(\"search\", END)\n",
    "graph.add_edge(\"ask_user\", END)\n",
    "\n",
    "runnable = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> run_query_agent\n",
      "> router\n",
      "> run_ask_user_preferences\n",
      "action: [ToolAgentAction(tool='ask_user', tool_input={'statement': 'Do you have any preferences for the apartment?'}, log=\"\\nInvoking: `ask_user` with `{'statement': 'Do you have any preferences for the apartment?'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'function': {'arguments': '{\"statement\":\"Do you have any preferences for the apartment?\"}', 'name': 'ask_user'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 311, 'total_tokens': 333}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2b2d3c5e-17ac-4a68-afbf-411aeb3bdeb8-0', tool_calls=[{'name': 'ask_user', 'args': {'statement': 'Do you have any preferences for the apartment?'}, 'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 22, 'total_tokens': 333})], tool_call_id='call_J4z3FDbqNXtstKsuwchlrWf5')]\n",
      "tool_call: {'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'function': {'arguments': '{\"statement\":\"Do you have any preferences for the apartment?\"}', 'name': 'ask_user'}, 'type': 'function'}\n",
      "out: {'statement': 'Do you have any preferences for the apartment?'}\n",
      "{'input': 'I am looking for apartment', 'agent_out': [ToolAgentAction(tool='ask_user', tool_input={'statement': 'Do you have any preferences for the apartment?'}, log=\"\\nInvoking: `ask_user` with `{'statement': 'Do you have any preferences for the apartment?'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'function': {'arguments': '{\"statement\":\"Do you have any preferences for the apartment?\"}', 'name': 'ask_user'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 311, 'total_tokens': 333}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2b2d3c5e-17ac-4a68-afbf-411aeb3bdeb8-0', tool_calls=[{'name': 'ask_user', 'args': {'statement': 'Do you have any preferences for the apartment?'}, 'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 22, 'total_tokens': 333})], tool_call_id='call_J4z3FDbqNXtstKsuwchlrWf5')], 'intermediate_steps': [{'statement': \"{'statement': 'Do you have any preferences for the apartment?'}\"}]}\n",
      "[ToolAgentAction(tool='ask_user', tool_input={'statement': 'Do you have any preferences for the apartment?'}, log=\"\\nInvoking: `ask_user` with `{'statement': 'Do you have any preferences for the apartment?'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'function': {'arguments': '{\"statement\":\"Do you have any preferences for the apartment?\"}', 'name': 'ask_user'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 311, 'total_tokens': 333}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2b2d3c5e-17ac-4a68-afbf-411aeb3bdeb8-0', tool_calls=[{'name': 'ask_user', 'args': {'statement': 'Do you have any preferences for the apartment?'}, 'id': 'call_J4z3FDbqNXtstKsuwchlrWf5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 22, 'total_tokens': 333})], tool_call_id='call_J4z3FDbqNXtstKsuwchlrWf5')]\n"
     ]
    }
   ],
   "source": [
    "out = runnable.invoke({\n",
    "    \"input\": \"I am looking for apartment\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(out)\n",
    "print(out[\"agent_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
